str(rikken[[1]])
str(rikken_text[[1]])
#ツイートの中にも、IDやURLなど邪魔な情報が含まれているので削除。
rikken_text %<>% str_replace_all("\\p{ASCII}","")
## Mac ないし Linux の場合。※なぜかいける。
tweet(date())
# 欠損値があれば、削除。
rikken_text <- rikken_text[!is.na(texts)]
# 欠損値があれば、削除。
rikken_text <- rikken_text[!is.na(rikken_text)]
# 文字列を全部結合させる。（複数の文章を一つのベクトルにする感じ）
rikken_text2 <- paste(rikken_text, collapse ="")
?write
rikken_text2 %>% head()
?tempfile
xfile <- tempfile()
write(rikken_text2, xfile)
library(RMeCab)
rikken_df <- docDF(xfile, type = 1, pos = "名詞")
#library(magrittr) # %<>% 演算子と ! を利用する
# 非自立、数、サ変を省く。（!をうまく使う）
mext %<>% filter(!POS2 %in% c("非自立", "数","サ変接続"))
#library(magrittr) # %<>% 演算子と ! を利用する
# 非自立、数、サ変を省く。（!をうまく使う）
rikken_df %<>% filter(!POS2 %in% c("非自立", "数","サ変接続"))
head(rikken_df)
tail(rikken_df)
# 列名を変更
## everything()で全ての列を抽出。
## starts_with()で、"file"で始まる列名を、FREQによって置き換える。
rikken_df %<>% select(everything(), FREQ = starts_with("file"))
head(rikken_df)
unlink(xfile) #一時ファイルを削除
rikken_df %>% arrange(FREQ) %>% tail(40)
rikken_df %>% arrange(FREQ) %>% tail(80)
wordcloud (rikken_df$TERM, rikken_df$FREQ, min.freq = 7, family = "JP1")
# termとpos1が同じでもpos2が違うくて同じ語が2個以上出現している。
# 一つにまとめる。
nrow(rikken_df)
# termとpos1が同じでもpos2が違うくて同じ語が2個以上出現している。
# 一つにまとめる。
nrow(rikken_df)
rikken_df2 <- rikken_df%>% select(TERM, POS1, FREQ) %>%
group_by(TERM, POS1) %>%
summarize(FREQ = sum(FREQ))
nrow(rikken_df2)
wordcloud (rikken_df2$TERM, rikken_df2$FREQ, min.freq = 7, family = "JP1")
rikken_df2 %>% filter(TERM == "\\p{ASCII}")
rikken_df2 %>% filter(TERM == "U")
rikken_df2 %>% filter(TERM == "\\p{ASCII}")
#################
tail(rikken_df2)
#################
tail(rikken_df2,50)
#################
head(rikken_df2,10)
#################
rikken_df2 %>% arrange(FREQ) %>% tail(50)
#################
rikken_df2 %>% arrange(FREQ) %>% tail(50)
#################
rikken_df2 %>% arrange(FREQ) %>% tail(50)
rikken_df2 %>% filter(TERM == "\\p{ASCII}")
#################
rikken_df2 %>% arrange(FREQ) %>% tail(50)
wordcloud (rikken_df2$TERM, rikken_df2$FREQ, min.freq = 7, family = "JP1")
# あとで使えるように保存
write.table(rikken_df2, "rikken_df2.txt")
wordcloud (rikken_df2$TERM, rikken_df2$FREQ, min.freq = 7, family = "JP1")
# install.packages("languageR", dependencies = TRUE)
library(languageR)
# データセットの準備
data(alice)
# データセットの冒頭20語の確認
head(alice, 20)
# テキストファイルからのデータ読み込み（Obama.txtを選択）
text.data <- scan(file.choose(), what = "char", sep = "\n", quiet = TRUE)
getwd()
setwd("C:/Users/rstud/Documents/GitHub/R/TextMining02/data")
# 単語ベクトルの作成
word.vector <- unlist(strsplit(text.data, "\\W"))
# スペースを削除
not.blank <- which(word.vector != "")
obama <- word.vector[not.blank]
# データの確認
head(obama, 20)
# インターネット上のデータの読み込み
text.data <- scan("http://www.xxx/yyy.txt", what = "char", sep = "\n", quiet = TRUE)
# 分析テキストの指定
word.vector <- alice
# 大文字を小文字に変換
word.vector.lower <- tolower(word.vector)
# 検索語の生起位置を取得（ここでは，"rabbit"）
word.positions <- which(word.vector.lower == "rabbit")
# 検索語の前後何語まで表示するかを指定（ここでは，5語）
context <- 5
# KWICコンコーダンスの作成
for(i in seq(word.positions)) {
if(word.positions[i] == 1) {
before <- NULL
} else {
start <- word.positions[i] - context
start <- max(start, 1)
before <- word.vector.lower[start : (word.positions[i] - 1)]
}
end <- word.positions[i] + context
after <- word.vector.lower[(word.positions[i] + 1) : end]
after[is.na(after)] <- ""
keyword <- word.vector.lower[word.positions[i]]
cat("--------------------", i, "--------------------", "\n")
cat(before, "[", keyword, "]", after, "\n")
}
# 検索語の生起位置を視覚化
# ある語が全体を通して使われているのか、一部だけに登場するのかが分かる。
# 今回のコンコーダンスプロットでは、「rabbit」は前半と後半で多く使われているとわかった。
plot(word.vector.lower == "rabbit", type = "h", yaxt = "n", main = "rabbit")
# install.packages("tm", dependencies = TRUE)
library(tm)
# 数字と句読点の削除
corpus.cleaned <- removeNumbers(word.vector.lower)
corpus.cleaned <- removePunctuation(corpus.cleaned)
# スペースを削除
not.blank <- which(corpus.cleaned != "")
corpus.cleaned <- corpus.cleaned [not.blank]
# 頻度表の作成
freq.list <- table(corpus.cleaned)
sorted.freq.list <- sort(freq.list, decreasing = TRUE)
sorted.table <- paste(names(sorted.freq.list), sorted.freq.list, sep = ": ")
# 頻度表（頻度上位20位まで）の確認
head(sorted.table, 20)
# ストップワードを個別に設定（ここでは，"the"と"and"を除外）
corpus.cleaned.2 <- removeWords(corpus.cleaned, c("the", "and"))
# スペースを削除
not.blank <- which(corpus.cleaned.2 != "")
corpus.cleaned.2 <- corpus.cleaned.2[not.blank]
# ストップワードを個別に設定（ここでは，"the"と"and"を除外）
corpus.cleaned.2 <- removeWords(corpus.cleaned, c("the", "and"))
# スペースを削除
not.blank <- which(corpus.cleaned.2 != "")
corpus.cleaned.2 <- corpus.cleaned.2[not.blank]
# 頻度表の作成
freq.list.2 <- table(corpus.cleaned.2)
sorted.freq.list.2 <- sort(freq.list.2, decreasing = TRUE)
sorted.table.2 <- paste(names(sorted.freq.list.2), sorted.freq.list.2, sep = ": ")
# 頻度表（頻度上位20位まで）の確認
head(sorted.table.2, 20)
# 語幹処理
corpus.cleaned.3 <- stemDocument(corpus.cleaned)
# 語幹処理
corpus.cleaned.3 <- stemDocument(corpus.cleaned)
# 頻度表の作成
freq.list.3 <- table(corpus.cleaned.3)
sorted.freq.list.3 <- sort(freq.list.3, decreasing = TRUE)
sorted.table.3 <- paste(names(sorted.freq.list.3), sorted.freq.list.3, sep = ": ")
# 頻度表（頻度上位20位まで）の確認
head(sorted.table.3, 20)
library(wordcloud)
wordcloud(corpus.cleaned, min.freq = 5, random.order = FALSE)
wordcloud(corpus.cleaned, min.freq = 5, random.order = FALSE)
# 2-gramsの抽出
ngrams <- paste(corpus.cleaned[1 : (length(corpus.cleaned) - 1)], corpus.cleaned[2 : length(corpus.cleaned)])
# 2-gramsの抽出
ngrams <- paste(corpus.cleaned[1 : (length(corpus.cleaned) - 1)], corpus.cleaned[2 : length(corpus.cleaned)])
# 頻度集計
ngram.freq <- table(ngrams)
sorted.ngram.freq <- sort(ngram.freq, decreasing = TRUE)
sorted.ngram.table <- paste(names(sorted.ngram.freq), sorted.ngram.freq, sep = ": ")
# 頻度上位20位までを表示
head(sorted.ngram.table, 20)
# 共起語の頻度分析
# 検索語の指定（ここでは，"rabbit"）
search.word <- "\\brabbit\\b"
# スパンの指定（ここでは，前後2語まで）
span <- 2
span <- (-span : span)
# 出力ファイル名の指定（ここでは，output.txt）
output.file <- "output.txt"
# 検索語の出現する位置を特定
positions.of.matches <- grep(search.word, corpus.cleaned, perl = TRUE)
# 共起語の集計
results <- list()
for(i in 1 : length(span)) {
collocate.positions <- positions.of.matches + span[i]
collocates <- corpus.cleaned[collocate.positions]
sorted.collocates <- sort(table(collocates), decreasing = TRUE)
results[[i]] <- sorted.collocates
}
# 集計表のヘッダーを出力
cat(paste(rep(c("W_", "F_"), length(span)), rep(span, each = 2), sep = ""), "\n", sep = "\t", file = output.file)
# 集計データを出力
lengths <- sapply(results, length)
for(k in 1 : max(lengths)) {
output.string <- paste(names(sapply(results, "[", k)), sapply(results, "[", k), sep = "\t")
output.string.2 <- gsub("NA\tNA", "\t", output.string, perl = TRUE)
cat(output.string.2, "\n", sep = "\t", file = output.file, append = TRUE)
}
# install.packages("koRpus", dependencies = TRUE)
library(koRpus)
# install.packages("koRpus", dependencies = TRUE)
library(koRpus)
# テキストの読み込み（Obama.txtを選択）
tok <- tokenize(file.choose(), lang = "en")
# 異語率の計算
TTR(tok)
# 異語率の計算
TTR(tok)
# ギロー指数の計算
R.ld(tok)
# ギロー指数の計算
R.ld(tok)
# MATTRの計算
MATTR(tok)
# MATTRの計算
MATTR(tok)
# MATTRの計算
MATTR(tok)
# MTLDの計算
MTLD(tok)
# Flesch-Kincaid Grade Levelの計算
flesch.kincaid(tok)
# Coleman-Liau Indexの計算
coleman.liau(tok)
# MATTRの計算
MATTR(tok)
# MTLDの計算
MTLD(tok)
# Flesch-Kincaid Grade Levelの計算
flesch.kincaid(tok)
# Coleman-Liau Indexの計算
coleman.liau(tok)
# Automated Readability Indexの計算
ARI(tok)
library(RMeCab)
library(dplyr)
library(stringr)
names <- c("岡本 雄輝","田中 謙吾","星野 ヨシキ","Michael Jackson","Jay Gatsby","Tom Buchanan","Monster Hunter","間 一紘","Alex Yuan")
names %>% str_replace("(\\w+) (\\w+)","\\1")
names %>% str_replace("(\\w+) (\\w+)","\\2")
##########思った通りにならん
names2 <- c("a b c","d e f","g h i","j k l","m n o","p q r","s t u")
names2 %>% str_replace("(\\W) (\\w) (\\w)","\\1")
x <- "ひらがな hiragana カタカナ katakana 日本語 123456"
x %>% str_replace_all("\\p{ASCII}","") ##ASCII文字(英数字や半角記号)を空白に置換
x %>% str_replace_all("\\p{Hiragana}","")  ##ひらがなを空白に置換
x %>% str_replace_all("\\p{Katakana}","")  ##カタカナを空白に置換
x %>% str_replace_all("\\p{ASCII}","") ##ASCII文字(英数字や半角記号)を空白に置換
x %>% str_replace_all("\\p{Hiragana}","")  ##ひらがなを空白に置換
x %>% str_replace_all("\\p{Katakana}","")  ##カタカナを空白に置換
x %>% str_replace_all("\\p{Han}","")  ##漢字を空白に置換
library(tm)
getwd()
setwd("C:/Users/rstud/Documents/GitHub/R/TextMining")
?VCorpus
#「alice」というオブジェクトに「data/alice/」フォルダにあるすべてのデータを代入している。
# VCorpus関数は、テキストを一時的にメモリに保存。コーパスに変換。
alice <- VCorpus(DirSource(dir = "data/alice/"),
readerControl = list(language = "english"))
#「alice」というオブジェクトに「data/alice/」フォルダにあるすべてのデータを代入している。
# VCorpus関数は、テキストを一時的にメモリに保存。コーパスに変換。
alice <- VCorpus(DirSource(dir = "data/alice/"),
readerControl = list(language = "english"))
alice %>% inspect
inspect(alice)
# inspectで登録された情報を確認。
alice %>% inspect()
inspect(alice)
# テキストの本文を出力するには、as.character()
alice[[1]] %>% as.character()
as.character(alice[[1]])
alice[[2]] %>% as.character()
as.character(alice[[2]])
alice[[3]] %>% as.character()
as.character(alice[[3]])
# テキストの本文を出力するには、as.character()
alice[[1]] %>% as.character()
alice1 <- alice %>% tm_map(stripWhitespace)  ##余分な空白削除
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
library(magrittr)
# removePunctuation(alice1)、でも良い。
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %>% tm_map(content_transformer(tolower))
##この状態だと、alice1にこの行でだけ、命令を実行したことになる。
##上書き保存されていないから、他の行でalice1オブジェクトに命令をしても変化は起きない。
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
## 上書き保存「%<>%」すると変化が分かる。
alice1 %<>% tm_map(content_transformer(tolower))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %<>% tm_map(removeWords,stopwords("english"))
# removeWords(データ,stopwords("english"))
# removeWords(データ,c("the","and"))でもいける。
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
## ストップワードを削除・上書きした。しかし、削除部分が空白になっているため、空白削除の前にやるべき。
stripWhitespace(alice1)
## ストップワードを削除・上書きした。しかし、削除部分が空白になっているため、空白削除の前にやるべき。
stripWhitespace(alice1)
# removeWords(データ,stopwords("english"))
# removeWords(データ,c("the","and"))でもいける。
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
## ストップワードを削除・上書きした。しかし、削除部分が空白になっているため、空白削除の前にやるべき。
alice1 %<>% tm_map(stripWhitespace)
# removeWords(データ,stopwords("english"))
# removeWords(データ,c("the","and"))でもいける。
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
library(SnowballC)
## ステミング（変化形を語幹に戻してカウント）
alice1 %<>% tm_map(stemDocument)
alice1[[1]] %>% as.character()
## 単語文書行列（文書を単語単位に分解した結果をマトリックス化）
dtm <- TermDocumentMatrix(alice1)
## inspect()で確認できる
dtm %>% inspect()
## 3回以上出現した単語を出力。
dtm %>% findFreqTerms(3)
## alicと相関係数が0.8以上の単語のみ出力。
dtm %>% findAssocs("alic", 0.8)
library(RMeCab)
RMeCabC("本を読んだ")
RMeCabC("今日は良い天気だが、花粉が飛んでいて辛い")
library(dplyr)
## 出力結果が縦（リスト）だと見にくいから、「unlist」でベクトル化。
RMeCabC("今日は本を読んだ。テキストマイニングに関する本だ。とても面白い。") %>% unlist()
hon [names(hon) %in% c("名詞","動詞")]  ## 動詞が表層語（活用形）のまま
hon [names(hon) %in% c("名詞","動詞")]  ## 動詞が表層語（活用形）のまま
hon <- RMeCabC("今日は本を読んだ。") %>% unlist()
hon [names(hon) %in% c("名詞","動詞")]  ## 動詞が表層語（活用形）のまま
#
?names
#
names(hon)
#
hon [names(hon) %in% c("名詞","動詞")]  ## 動詞が表層語（活用形）のまま
# 第2引数で「1」を入れると、原形を出力。
RMeCabC("今日は本を読んだ",1) %>% unlist()
file.exists("data/hon.txt") ##ファイルがあるかを確認
RMeCabText("data/hon.txt") ##RMeCabTextで名詞・動詞だけでなく全てを出力。
x # 解析結果を確認
tmp <- tempfile() ##ファイルを一時的に作成（後に自動で削除される。）
writeLines("本を買った", con = tmp)
tmp # 一時ファイルの保存場所を表示
x <-RMeCabText(tmp)
unlink(tmp) # 一時ファイルを削除
x # 解析結果を確認
library(purrr)
x %>% map_chr(extract(9)) #xの各リストの9番目（読み方）を抽出。
######################################### ここよくわからん。 ####################################
tmp <- data.frame(BUN = "本を買った", stringAsFactor = FALSE)
x <- docDF(tmp, "BUN", type = 1)
x
######メロスメロスメロスメロスメロスメロスメロスメロスメロスメロスメロスメロスメロスメロス###########
merosu <- RMeCabFreq("data/merosu.txt")
merosu %>% head(20)
##デフォルトでは文字単位で解析になる。「type=1」は形態素解析。
## TERM数が1063個あることが分かる。
merosu <- docDF("data/merosu.txt",
type=1 , pos=c("名詞","形容詞","動詞"))
merosu %>% head(10) ##「merosu.txt」が語の出現頻度。分かりにくいから変える。
library(magrittr)
#出現頻度の列名を「FREQ」に変えて、上書き保存して、FREQの小さい順に並び替えた。
merosu %<>% rename(FREQ = merosu.txt) %>% arrange(FREQ)
merosu %>% tail()
merosu %>% filter(TERM == "メロス")  ##「メロス」の出現回数を表示。
merosu2 <- merosu %>% select(TERM, POS1, FREQ) %>%  ## 表示カラムからPOS2（品詞細分類）を削除。
group_by(TERM, POS1) %>%      ## POS2の表示を消しても裏側では細分類で区別されていて「メロス」は2つのまま。selectだけでは変わらない。
## group_byでTERMとPOS1が一致する行のFREQ数を一緒にする（FREQ数は同じになるが、POS2が異なると両方出力される）
summarize(FREQ = sum(FREQ))   ## 単語とPOS1（品詞大分類）のペアが2行以上存在する行（POS2が異なるから）をPOS2を無視して合体させる。
## FREQという行を作って（今回は上書き）、そこに2行以上存在したペアの合計値を代入した。
merosu2 %>% NROW() ##1063あったTERMが30ぐらい統合された。
merosu2 %>% filter(TERM == "メロス")
## group_byでPOS1でグループ化（TERMは無視）
## その合計（名詞・動詞・形容詞などの合計頻度）を「SUM」というカラムに代入。
merosu2 %>% group_by(POS1) %>% summarize(SUM = sum(FREQ))
## mutate()で割合を出す。
## 約6割が名詞であることが分かる。
merosu2 %>% group_by(POS1) %>% summarize(SUM = sum(FREQ)) %>% mutate(PROP = SUM / sum(SUM))
##品詞情報を指定して検索
merosu %>% filter(POS1 %in% c("動詞","形容詞"), POS2 == "自立") %>% NROW()
## 共起語の解析。第1引数にファイル名。第2引数にnode。第3引数にspan。
res <- collocate("data/kumo.txt", node = "極楽", span = 3)
#[morphems]は、形態素の種類の数。
#[tokens]は、総単語数。
res %>% tail(15)
log2(4/((4/1808)*3*2*10))  ##ノード「極楽」と共起語「蓮池」のMI値。1.58以上なので共起関係あり。
res <- collScores(res, node = "極楽", span = 3) # 「collScores」は、T値とMI値両方出してくれる。
res %>% tail(15)
log2(4/((4/1808)*3*2*10))  ##ノード「極楽」と共起語「蓮池」のMI値。1.58以上なので共起関係あり。
log2(4/((4/1808)*3*2*10))  ##ノード「極楽」と共起語「蓮池」のMI値。1.58以上なので共起関係あり。
res <- collScores(res, node = "極楽", span = 3) # 「collScores」は、T値とMI値両方出してくれる。
res %>% tail(15)
# 「collScores」は、T値とMI値両方出してくれる。
# T値は、平均値の差の検定。しかし、母集団を正規分布と仮定するため、言語処理には向かないという批判もある。
# T = 実測値-期待値/実測値の平方根。1.65以上なら共起が偶然ではないと考えられる。
# MI値は、
res <- collScores(res, node = "極楽", span = 3)
res %>% tail(15)
####################単語文書行列単語文書行列単語文書行列単語文書行列単語文書行列#####################
mat <- docMatrix("data/doc") #docフォルダにあるデータを全て代入
## 形態素解析をするために単語文書行列を作った。しかし2つのメタ情報は解析には必要ないから削除。
mat <- mat[ rownames(mat) != "[[LESS-THAN-1]]", ]
mat <- mat[ rownames(mat) != "[[TOTAL-TOKENS]]", ]
# docMatrix()のデフォルトでは「名詞・形容詞」のみ。だからposで指定する。
# 単語（形態素）には、内容語と機能語がある。
# 内容語は名詞・動詞・形容詞など、文書の内容を直接表現するもの。複数の文書をテーマごとに分類できる。
# 機能語は助詞など。書き手の文体を解析できる。
mat <- docMatrix("data/doc", pos = c("名詞","形容詞","動詞","助詞"))
mat
# 局所的重みをtf(単語総頻度)、大域的重みをidf(単語が出現した文書数)と指定し、正規化（norm）している。
matW <- docMatrix("data/doc", weight = "tf*idf*norm")
# 単語文書行列をする時に、それぞれの文章の長さ（データ量）が異なると、解析に影響が出る。それを調整するために、「正規化」する。
# 局所的重みをtf(単語の出現頻度)、大域的重みをidf(単語が出現した文書数)と指定し、正規化（norm）している。
matW <- docMatrix("data/doc", weight = "tf*idf*norm")
matW %>% head()
options(digits = 3) # 少数点以下3位まで表示
matW %>% head()
###########################よくわからん##########################
# applyの第1引数はデータ（今回はパイプ処理しているからナシ）
# 第2引数は、「１」ならば行単位で処理、「２」ならば列単位で処理。
# 第3引数で処理を指定。行列の列ごとに要素を2倍。
matW %>% apply(2, function(x) sum(x^2))
?docMatrix
# データフレームからテキスト解析を行う場合、文字列が因子に変換されないように「stringAsFactor=FALSE」とすべし。
photo <- read.csv("data/photo.csv", stringsAsFactor = FALSE)
photo
res <- docMatrixDF(photo[ ,"Reply"]) # photoデータのReplyカラムだけを抽出して、単語文書行列を作成。
# 単語文書行列を作れる関数は「docMatrix()」「docMatrix2()」「docMatrixDF()」のみ。
res
# docDFは、「Nグラム」を作る関数。
# 「Nグラム」は、単語文書行列（形態素解析の結果をデータフレーム化したもの）になっているデータから作成
# だから、第1引数に単語文書行列のデータを取る。
# 第2引数は、単語文書"行列"なので、どの列のNグラムを作るかを指定。
# 第3引数に、形態素解析（１）か文字単位の解析（０）かを選ぶ。
# Nグラムに使う品詞を「pos」で指定。デフォルトは形態素全部。
res <- docDF(photo, column = "Reply", type = 1, pos = c("名詞","形容詞"), N = 2)
res #名詞・形容詞が2語連続で連なっている組み合わせは、この1つだけだった。
photo
res #名詞・形容詞が2語連続で連なっている組み合わせは、この1つだけだった。
# Sというオブジェクトに「メロスは激怒した」という文章をデータフレーム化して、文字列として代入。
S <- data.frame(BUN = "メロスは激怒した", stringsAsFactors = FALSE)
# データフレーム化したSをでNグラムを作成。「docDF」はNグラムを作る関数。
# pos指定がないので全ての形態素。
# columnは列名でも列番号でもOK。
(docDF(S, column = 1, type = 1, N = 2))
?docDF
## Nグラムを作れる関数は「docDF()」「Ngram()」「NgramDF()」など。
merosu <- Ngram("data/merosu.txt", type = 0, N = 2) # 「type=0」なので、文字単位の解析になる。
merosu %>% head() #助詞と句点で書き手を分類することができる。
merosu %>% head() #助詞と句点で書き手を分類することができる。
merosu <- Ngram("data/merosu.txt", type = 1, N = 2) # 「type=1」なので、形態素単位の解析になる。
merosu %>% head(20) #助詞と句点で書き手を分類することができる。
merosu <- Ngram("data/merosu.txt", type = 2, N = 2) # 「type=2」で品詞を表示。
merosu %>% head()
# 「nDF=1」はTERMで纏められている2語をN1とN2に分けて出力
merosu <- docDF("data/merosu.txt", type = 1, pos = c("名詞","形容詞"), N = 2, nDF = 1)
merosu %>% head(20)
#今までのNグラム出力はデータフレームだった。しかし、分析関数によっては、データ形式が行列でなければならない。
# docNgramはNグラムデータを行列で出力。
merosu <- docNgram("data/doc")
merosu %>% head()
merosu %>% rownames()
# NgramDF()は、「ネットワークグラフ」を作成する場合に使う。
merosu <- NgramDF("data/merosu.txt", type = 1, N = 2) # 「type=1」なので、形態素単位の解析になる。
merosu %>% head() # pos指定がないから全ての形態素。
# docNgram2はメモリ改善されている。docNgramとの違いは、デフォルトで「type=1」で文字単位の解析・バイグラム
res <- docNgram2("data/doc") # デフォルトでは文字単位の解析・バイグラムになっている。
res <- docNgram2("data/doc", type = 1, pos = c("名詞","形容詞")) #「type=1」で形態素のバイグラム。
res <- docNgram2("data/doc",type = 1,
pos = c("名詞","形容詞","記号")) # 句読点は「記号」と分類されている。
res %>% head(15)
getwd()
library(ggplot2) # Data visualisation
library(ggrepel) # data visualisation
library(dplyr) # data manipulation
library(viridis) # data visualisation
library(stringr) # String manipulation
library(RColorBrewer) # Color palette
library(ggthemes) # Themes for plot
library(tidyverse)
library(wordcloud2)
library(zoo)
library(anytime)
library(data.table)
library(treemap)
library(cowplot)
library(wordcloud)
install.packages("ggplot2")
install.packages("ggrepel")
library(rvest)
# read_html("URL")で、URL先のページ情報を全て取得する。
jobs <- read_html("http://bookmeter.com/b/4062180731")
library(dplyr)
# さっきのURLだと書評だけでなく、書籍のあらすじなども取得されている。
# jobsオブジェクトに代入された全ての情報から書評テキスト（ノード）だけを抽出。
# ノードだけを抽出するには、「html_nodes()」を使う。
# ノードの中からテキストだけを抽出。「html_text()」
# read_html() → html_nodes() → html_text()
reviews <- jobs %>% html_nodes("div[id^='review_text_']") %>% html_text()
library(magrittr)
install.packages("ggplot2")
install.packages("ggplot2")
