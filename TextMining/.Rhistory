names2 %>% str_replace("(\\W+) (\\w+) (\\w+)","\\1")
names %>% str_replace("(\\w+) (\\w+)","\\2")
names2 <- c("a b c","d e f","g h i","j k l","m n o","p q r","s t u")
names2 %>% str_replace("(\\W+) (\\w+) (\\w+)","\\1")
names2 %>% str_replace("(\\W+) (\\w+)","\\1")
names2 <- c("a b c","d e f","g h i","j k l","m n o","p q r","s t u")
names2 %>% str_replace("(\\W+) (\\w+)","\\1")
##########思った通りにならん
names2 <- c("a1 b1 c1","d1 e1 f1","g1 h1 i1","j1 k1 l1","m1 n1 o1","p1 q1 r1","s1 t1 u1")
names2 %>% str_replace("(\\W+) (\\w+) (\\w+)","\\1")
names2 %>% str_replace("() () ()","\\1")
names2 %>% str_replace("() () ()","\\1")
##########思った通りにならん
names2 <- c("a b c","d e f","g h i","j k l","m n o","p q r","s t u")
names2 %>% str_replace("() () ()","\\1")
##########思った通りにならん
names2 <- c("a b c","d e f","g h i","j k l","m n o","p q r","s t u")
names2 %>% str_replace("(\\W) (\\w) (\\w)","\\1")
x <- "ひらがな hiragana カタカナ katakana 日本語 123456"
x <- "ひらがな hiragana カタカナ katakana 日本語 123456"
x %>% str_replace_all("\\p{ASCII}","") ##ASCII文字を空白に置換
x %>% str_replace_all("\\p{Hiranaga}","")  ##ひらがなを空白に置換
x %>% str_replace_all("\\p{ASCII}","") ##ASCII文字(英数字や半角記号)を空白に置換
x %>% str_replace_all("\\p{Hiranaga}","")  ##ひらがなを空白に置換
x %>% str_replace_all("\\p{Katakana}","")  ##カタカナを空白に置換
x %>% str_replace_all("\\p{Hiragana}","")  ##ひらがなを空白に置換
x %>% str_replace_all("\\p{Katakana}","")  ##カタカナを空白に置換
x %>% str_replace_all("\\p{Han}","")  ##
x %>% str_replace_all("\\p{Han}","")  ##漢字を空白に置換
name %>% str_replace_all("\\p{ASCII}","") ##ASCII文字(英数字や半角記号)を空白に置換
names %>% str_replace_all("\\p{ASCII}","") ##ASCII文字(英数字や半角記号)を空白に置換
names %>% str_replace_all("\\p{ASCII}","") ##ASCII文字(英数字や半角記号)を空白に置換
names %>% str_replace_all("\\p{Hiragana}","")  ##ひらがなを空白に置換
names %>% str_replace_all("\\p{Katakana}","")  ##カタカナを空白に置換
names %>% str_replace_all("\\p{Han}","")  ##漢字を空白に置換
library(tm)
getwd
getwd()
alice <- VCorpus(DirSource(dir = "data/alice/"),
readerControl = list(language = "english"))
alice %>% inspect()
alice %>% inspect
inspect(alice)
alice%>% as.character()
alice %>% as.character()
alice1 %>% as.character()
alice[[1]] %>% as.character()
as.character(alice[[1]])
as.character(alice[[2]])
alice[[2]] %>% as.character()
alice %>% as.character()
alice[[3]] %>% as.character()
as.character(alice[[3]])
alice1 <- alice %>% tm_map(stripWhitespace)
alice1
alice1 %>% as.character()
alice1[[1]] %>% as.character()
alice1[[3]] %>% as.character()
alice1[[2]] %>% as.character()
alice[[2]] %>% as.character()
library(magrittr)
alice1 %<>% tm_map(removePunctuation)
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %>% tm_map(content_transformer(tolower))
alice1 %>% tm_map(content_transformer(tolower))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %>% tm_map(content_transformer(tolower))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %>% tm_map(content_transformer(tolower))
alice1[[1]] %>% as.character()
## 上書き保存「%<>%」すると変化が分かる。
alice1 %<>% tm_map(content_transformer(tolower))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %<>% tm_map(removeWords,stopwords("english"))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
library(SnowballC)
alice1 %<>% as.character()
alice[[1]] %<>% as.character()
#「alice」というオブジェクトに「data/alice/」フォルダにあるすべてのデータを代入している。
alice <- VCorpus(DirSource(dir = "data/alice/"),
readerControl = list(language = "english"))
alice %>% inspect
inspect(alice)
alice[[1]] %>% as.character()
as.character(alice[[1]])
alice[[2]] %>% as.character()
as.character(alice[[2]])
alice[[3]] %>% as.character()
as.character(alice[[3]])
alice1 <- alice %>% tm_map(stripWhitespace)  ##余分な空白削除
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
library(magrittr)
alice1 %<>% tm_map(removePunctuation) #ピリオド、カンマ、クエスチョンマーク、括弧などを削除
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %>% tm_map(content_transformer(tolower))
##この状態だと、alice1にこの行でだけ、命令を実行したことになる。
##上書き保存されていないから、他の行でalice1オブジェクトに命令をしても変化は起きない。
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
## 上書き保存「%<>%」すると変化が分かる。
alice1 %<>% tm_map(content_transformer(tolower))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %<>% tm_map(removeWords,stopwords("english"))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
library(SnowballC)
alice[[1]] %>% as.character()
alice1 %<>%tm_map(stemDocument)
alice[[1]] %>% as.character()
#「alice」というオブジェクトに「data/alice/」フォルダにあるすべてのデータを代入している。
alice <- VCorpus(DirSource(dir = "data/alice/"),
readerControl = list(language = "english"))
alice %>% inspect
inspect(alice)
alice[[1]] %>% as.character()
as.character(alice[[1]])
alice[[2]] %>% as.character()
as.character(alice[[2]])
alice[[3]] %>% as.character()
as.character(alice[[3]])
alice1 <- alice %>% tm_map(stripWhitespace)  ##余分な空白削除
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %<>% tm_map(removePunctuation) #ピリオド、カンマ、クエスチョンマーク、括弧などを削除
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %>% tm_map(content_transformer(tolower))
##この状態だと、alice1にこの行でだけ、命令を実行したことになる。
##上書き保存されていないから、他の行でalice1オブジェクトに命令をしても変化は起きない。
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
## 上書き保存「%<>%」すると変化が分かる。
alice1 %<>% tm_map(content_transformer(tolower))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %<>% tm_map(removeWords,stopwords("english"))
alice1 %<>% tm_map(removeWords,stopwords("english"))
alice1[[1]] %>% as.character()
alice1 %<>% tm_map(removeWords,stopwords("english"))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %<>%tm_map(stemDocument)
alice[[1]] %>% as.character()
alice1[[1]] %>% as.character()
#「alice」というオブジェクトに「data/alice/」フォルダにあるすべてのデータを代入している。
alice <- VCorpus(DirSource(dir = "data/alice/"),
readerControl = list(language = "english"))
alice %>% inspect
inspect(alice)
alice[[1]] %>% as.character()
as.character(alice[[1]])
alice[[2]] %>% as.character()
as.character(alice[[2]])
alice[[3]] %>% as.character()
as.character(alice[[3]])
alice1 <- alice %>% tm_map(stripWhitespace)  ##余分な空白削除
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
library(magrittr)
alice1 %<>% tm_map(removePunctuation) #ピリオド、カンマ、クエスチョンマーク、括弧などを削除
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %>% tm_map(content_transformer(tolower))
##この状態だと、alice1にこの行でだけ、命令を実行したことになる。
##上書き保存されていないから、他の行でalice1オブジェクトに命令をしても変化は起きない。
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
## 上書き保存「%<>%」すると変化が分かる。
alice1 %<>% tm_map(content_transformer(tolower))
alice1[[1]] %>% as.character()
alice1[[2]] %>% as.character()
alice1[[3]] %>% as.character()
alice1 %<>% tm_map(removeWords,stopwords("english"))
alice1[[1]] %>% as.character()
alice1 %<>% tm_map(removeWords,stopwords("english"))
alice1 %<>%tm_map(stemDocument)
alice1[[1]] %>% as.character()
alice1 %<>% tm_map(stemDocument)
alice1[[1]] %>% as.character()
## 単語文書行列（文書を単語単位に分解した結果をマトリックス化）
dtm <- TermDocumentMatrix(alice1)
## inspect()で確認できる
dtm %>% inspect()
dtm%>% findFreqTerms(3)
dtm %>% findAssocs("alic", 0.8)
vignette("tm")
library(RMeCab)
RMeCabC("本を読んだ")
RMeCabC("今日は良い天気だが、花粉が飛んでいて辛い")
RMeCabC("今日は本を読んだ。テキストマイニングに関する本だ。とても面白い。") %>% unlist()
?names
hon [names(hon) %in% c("名詞","動詞")]
hon <- RMeCabC("今日は本を読んだ。") %>% unlist()
hon [names(hon) %in% c("名詞","動詞")]
RMeCabC("今日は本を読んだ",1)
RMeCabC("今日は本を読んだ",1) %>% unlist()
file.exists("data/hon.txt")
RMeCabCText("data/hon.txt")
RMeCabText("data/hon.txt")
RMeCabText("data/hon.txt") ##RMeCabTextで名詞・動詞だけでなく全てを出力。
install.packages("purrr")
tmp <- tempfile() ##ファイルを一時的に作成（後に自動で削除される。）
tmp <- tempfile() ##ファイルを一時的に作成（後に自動で削除される。）
writeLines("本を買った", con = tmp)
# 一時ファイルの保存場所を表示
tmp
x <-RMeCabText(tmp)
unlink(tmp)
# 解析結果を確認
x
x # 解析結果を確認
library(purrr)
x %>% map_chr(extract(9))
tmp <- data.frame(BUN = "本を買った", stringAsFactor = FALSE)
x <- docDF(tmp, "BUN", type = 1)
x
merosu <- RMeCabFreq("data/merosu.txt")
merosu %>% head()
merosu %>% head(20)
merosu <- docDF("data/merosu.txt",
type=1 , pos=c("名詞","形容詞","動詞"))
merosu %>% head()
merosu %>% head(10)
library(magrittr)
merosu %<>% renme(FREQ = merosu.txt) %>% arrange(FREQ)
merosu %<>% rename(FREQ = merosu.txt) %>% arrange(FREQ)
merosu %>% tail()
merosu %>% filter(TERM == "メロス")
merosu %>% filter(TERM == "メロス")  ##「メロス」の出現回数を表示。
merosu2 %>%NROW()
merosu2 %>% NROW()
merosu2 <- merosu %>% select(TREM, POS1, FREQ) %>%
group_by(TERM, POS1) %>%
summarize(FREQ = sum(FREQ))
merosu2 %>% NROW()
merosu2 <- merosu %>% select(TERM, POS1, FREQ) %>%
group_by(TERM, POS1) %>%
summarize(FREQ = sum(FREQ))
merosu2 %>% NROW()
merosu <- RMeCabFreq("data/merosu.txt")
merosu2 %>% NROW()
merosu2 %>% filter(TERM == "メロス")
merosu2 %>% group_by(POS1) %>% summarize(SUM = sum(FREQ))
## group_byでPOS1でグループ化（TERMは無視）
## その合計（名詞・動詞・形容詞などの合計頻度）を「SUM」というカラムに代入。
merosu2 %>% group_by(POS1) %>% summarize(SUM = sum(FREQ))
## mutate()で割合を出す。
merosu2 %>% group_by(POS1) %>% summarize(SUM = sum(FREQ)) %>% mutate(PROP = SUM / sum(SUM))
merosu2 %>% group_by(POS1)
merosu2["SUM"] <- sum(FREQ)
merosu2 %>% group_by(POS1) %>% merosu2["SUM"] <- sum(FREQ)
## mutate()で割合を出す。
merosu2 %>% group_by(POS1) %>% summarize(SUM = sum(FREQ)) %>% mutate(PROP = SUM / sum(SUM))
##品詞情報を指定して検索
merosu2 %>% filter(POS1 %in% c("動詞","形容詞"), POS2 == "自立") %>%
NROW()
##品詞情報を指定して検索
merosu2 %>% filter(POS1 %in% c("動詞","形容詞"), POS2 == "自立") %>% NROW()
##品詞情報を指定して検索
merosu2 %>% filter(POS1 %in% c("動詞","形容詞"), POS2 == "自立") %>% NROW()
##品詞情報を指定して検索
merosu %>% filter(POS1 %in% c("動詞","形容詞"), POS2 == "自立") %>% NROW()
##品詞情報を指定して検索
merosu %>% filter(POS1 %in% c("動詞","形容詞"), POS2 == "自立") %>% NROW()
merosu <- RMeCabFreq("data/merosu.txt")
merosu %>% head(20)
merosu <- docDF("data/merosu.txt",
type=1 , pos=c("名詞","形容詞","動詞")) ##デフォルトでは文字単位で解析になる。「type=1」は形態素解析。
merosu <- docDF("data/merosu.txt",
type=1 , pos=c("名詞","形容詞","動詞")) ##デフォルトでは文字単位で解析になる。「type=1」は形態素解析。
merosu %>% head(10) ##「merosu.txt」が語の出現頻度。分かりにくいから変える。
##品詞情報を指定して検索
merosu %>% filter(POS1 %in% c("動詞","形容詞"), POS2 == "自立") %>% NROW()
################蜘蛛の糸蜘蛛の糸蜘蛛の糸蜘蛛の糸蜘蛛の糸蜘蛛の糸蜘蛛の糸蜘蛛の糸#####################
res <- collocate("data/kumo.txt", node = "極楽", span = 3)
res
res %>% tail(15)
log2(4/((4/1808)*3*2*10))
res <- collScore(res, node = "極楽", span = 3)
res <- collScores(res, node = "極楽", span = 3)
res %>% tail(15)
####################単語文書行列単語文書行列単語文書行列単語文書行列単語文書行列#####################
mat <- docMatrix("data/doc") #docフォルダにあるデータを全て
mat
## 形態素解析をするために単語文書行列を作った。しかし2つのメタ情報は解析には必要ないから削除。
mat <- mat[ rownames(mat) != "[[LESS-THAN-1]]", ]
mat <- mat[ rownames(mat) != "[[TOTAL-TOKENS]]", ]
mat
mat <- docMatrix("data/doc", pos = c("名詞","形容詞","動詞","助詞"))
mat
matW <- docMatrix("data/doc", weight = "tf*idf*norm")
matW %>% head()
options(digits = 3) # 少数点以下3位まで表示
matW %>% head()
###########################よくわからん##########################
# applyの第1引数はデータ（今回はパイプ処理しているからナシ）
# 第2引数は、「１」ならば行単位で処理、「２」ならば列単位で処理。
# 第3引数で処理を指定。行列の列ごとに要素を2倍。
matW %>% apply(2, function(x) sum(x^2))
?docMatrix
photo <- read.csv("data/photo.csv", stringsAsFactor = FALSE)
photo
# 単語文書行列を作れる関数は「docMatrix()」「docMatrix2()」「docMatrixDF()」のみ。
res
res <- docMatrixDF(photo[ ,"Reply"]) # photoデータのReplyカラムだけを抽出して、単語文書行列を作成。
# 単語文書行列を作れる関数は「docMatrix()」「docMatrix2()」「docMatrixDF()」のみ。
res
S <- data.frame(BUN = "メロスは激怒した", stringsAsFactors = FALSE)
S
res <- docDF(photo, column = "Reply", type = 1, pos = c("名詞","形容詞"), N = 2)
photo
res <- docMatrixDF(photo[ ,"Reply"]) # photoデータのReplyカラムだけを抽出して、単語文書行列を作成。
# 単語文書行列を作れる関数は「docMatrix()」「docMatrix2()」「docMatrixDF()」のみ。
res
# photoオブジェクトには「data/photo.csv」のデータが文字列として代入されている。
# それをデータフレーム化して
res <- docDF(photo, column = "Reply", type = 1, pos = c("名詞","形容詞"), N = 2)
res <- docMatrixDF(photo[ ,"Reply"]) # photoデータのReplyカラムだけを抽出して、単語文書行列を作成。
# 単語文書行列を作れる関数は「docMatrix()」「docMatrix2()」「docMatrixDF()」のみ。
res
# photoオブジェクトには「data/photo.csv」のデータが文字列として代入されている。
# resにはphotoの「Reply」カラムだけ抽出して、単語文書行列を作っている。
# photoデータフレームのReplyカラムだけ抽出して、名詞・形容詞の形態素のみ抽出して、バイグラムを作成。
res <- docDF(photo, column = "Reply", type = 1, pos = c("名詞","形容詞"), N = 2)
res
S <- data.frame(BUN = "メロスは激怒した", stringsAsFactors = FALSE)
(docDF(S, column = 1, type = 1, N = 2))
?docDF
merosu <- Ngram("data/merosu.txt", type = 0, N = 2)
merosu %>% head()
## Nグラムを作れる関数は「docDF()」「Ngram()」「NgramDF()」など。
merosu <- Ngram("data/merosu.txt", type = 0, N = 2)
merosu <- Ngram("data/merosu.txt", type = 1, N = 2) # 「type=0」なので、文字単位の解析になる。
merosu %>% head() #助詞と句点で書き手を分類することができる。
merosu <- Ngram("data/merosu.txt", type = 1, N = 2) # 「type=1」なので、形態素単位の解析になる。
merosu %>% head() #助詞と句点で書き手を分類することができる。
merosu %>% head()
merosu <- Ngram("data/merosu.txt", type = 2, N = 2)
merosu %>% head()
merosu <- docDF("data/merosu.txt", type = 1, pos = c("名詞","形容詞"), N = 2, nDF = 1)
merosu %>% head(15)
merosu <- Ngram("data/merosu.txt", type = 1, N = 2) # 「type=1」なので、形態素単位の解析になる。
merosu %>% head(20) #助詞と句点で書き手を分類することができる。
merosu <- docDF("data/merosu.txt", type = 1, pos = c("名詞","形容詞"), N = 2, nDF = 1)
merosu %>% head(15)
merosu %>% head(20)
# 「nDF=1」はTERMで纏められている2語をN1とN2に分けて出力
merosu <- docDF("data/merosu.txt", type = 1, pos = c("名詞","形容詞"), N = 2, nDF = 1)
merosu %>% head(20)
library(RMeCab)
RMeCabC("本を読んだ")
RMeCabC("今日は良い天気だが、花粉が飛んでいて辛い")
library(dplyr)
## 出力結果が縦（リスト）だと見にくいから、「unlist」でベクトル化。
RMeCabC("今日は本を読んだ。テキストマイニングに関する本だ。とても面白い。") %>% unlist()
hon <- RMeCabC("今日は本を読んだ。") %>% unlist()
hon [names(hon) %in% c("名詞","動詞")]  ## 動詞が表層語（活用形）のまま
RMeCabC("今日は本を読んだ",1) %>% unlist() ## 第2引数で「1」を入れると、原形を出力。
file.exists("data/hon.txt") ##ファイルがあるかを確認
RMeCabText("data/hon.txt") ##RMeCabTextで名詞・動詞だけでなく全てを出力。
tmp <- tempfile() ##ファイルを一時的に作成（後に自動で削除される。）
writeLines("本を買った", con = tmp)
tmp # 一時ファイルの保存場所を表示
x <-RMeCabText(tmp)
unlink(tmp) # 一時ファイルを削除
x # 解析結果を確認
library(purrr)
x %>% map_chr(extract(9)) #xの各リストの9番目（読み方）を抽出。
######################################### ここよくわからん。 ####################################
tmp <- data.frame(BUN = "本を買った", stringAsFactor = FALSE)
x <- docDF(tmp, "BUN", type = 1)
x %>% map_chr(extract(9)) #xの各リストの9番目（読み方）を抽出。
library(purrr)
x %>% map_chr(extract(9)) #xの各リストの9番目（読み方）を抽出。
######################################### ここよくわからん。 ####################################
tmp <- data.frame(BUN = "本を買った", stringAsFactor = FALSE)
x <- docDF(tmp, "BUN", type = 1)
x
######メロスメロスメロスメロスメロスメロスメロスメロスメロスメロスメロスメロスメロスメロス###########
merosu <- RMeCabFreq("data/merosu.txt")
merosu %>% head(20)
##デフォルトでは文字単位で解析になる。「type=1」は形態素解析。
## TERM数が1063個あることが分かる。
merosu <- docDF("data/merosu.txt",
type=1 , pos=c("名詞","形容詞","動詞"))
merosu %>% head(10) ##「merosu.txt」が語の出現頻度。分かりにくいから変える。
library(magrittr)
#出現頻度の列名を「FREQ」に変えて、上書き保存して、FREQの小さい順に並び替えた。
merosu %<>% rename(FREQ = merosu.txt) %>% arrange(FREQ)
merosu %>% tail()
merosu %>% filter(TERM == "メロス")  ##「メロス」の出現回数を表示。
merosu2 <- merosu %>% select(TERM, POS1, FREQ) %>%  ## 表示カラムからPOS2（品詞細分類）を削除。
group_by(TERM, POS1) %>%      ## POS2の表示を消しても裏側では細分類で区別されていて「メロス」は2つのまま。selectだけでは変わらない。
## group_byでTERMとPOS1が一致する行のFREQ数を一緒にする（FREQ数は同じになるが、POS2が異なると両方出力される）
summarize(FREQ = sum(FREQ))   ## 単語とPOS1（品詞大分類）のペアが2行以上存在する行（POS2が異なるから）をPOS2を無視して合体させる。
## FREQという行を作って（今回は上書き）、そこに2行以上存在したペアの合計値を代入した。
merosu2 %>% NROW() ##1063あったTERMが30ぐらい統合された。
merosu2 %>% filter(TERM == "メロス")
## group_byでPOS1でグループ化（TERMは無視）
## その合計（名詞・動詞・形容詞などの合計頻度）を「SUM」というカラムに代入。
merosu2 %>% group_by(POS1) %>% summarize(SUM = sum(FREQ))
## mutate()で割合を出す。
## 約6割が名詞であることが分かる。
merosu2 %>% group_by(POS1) %>% summarize(SUM = sum(FREQ)) %>% mutate(PROP = SUM / sum(SUM))
##品詞情報を指定して検索
merosu %>% filter(POS1 %in% c("動詞","形容詞"), POS2 == "自立") %>% NROW()
## 共起語の解析。第1引数にファイル名。第2引数にnode。第3引数にspan。
res <- collocate("data/kumo.txt", node = "極楽", span = 3)
#[morphems]は、形態素の種類の数。
#[tokens]は、総単語数。
res %>% tail(15)
log2(4/((4/1808)*3*2*10))  ##ノード「極楽」と共起語「蓮池」のMI値。1.58以上なので共起関係あり。
res <- collScores(res, node = "極楽", span = 3) # 「collScores」は、T値とMI値両方出してくれる。
res %>% tail(15)
####################単語文書行列単語文書行列単語文書行列単語文書行列単語文書行列#####################
mat <- docMatrix("data/doc") #docフォルダにあるデータを全て代入
mat #　単語文書行列の解析結果は行列サイズが大きいため、そのまま出力するのは良くない（今回は小さいからOK）
## 形態素解析をするために単語文書行列を作った。しかし2つのメタ情報は解析には必要ないから削除。
mat <- mat[ rownames(mat) != "[[LESS-THAN-1]]", ]
mat <- mat[ rownames(mat) != "[[TOTAL-TOKENS]]", ]
# docMatrix()のデフォルトでは「名詞・形容詞」のみ。だからposで指定する。
# 単語（形態素）には、内容語と機能語がある。
# 内容語は名詞・動詞・形容詞など、文書の内容を直接表現するもの。複数の文書をテーマごとに分類できる。
# 機能語は助詞など。書き手の文体を解析できる。
mat <- docMatrix("data/doc", pos = c("名詞","形容詞","動詞","助詞"))
mat
# 局所的重みをtf(単語総頻度)、大域的重みをidf(単語が出現した文書数)と指定し、正規化（norm）している。
matW <- docMatrix("data/doc", weight = "tf*idf*norm")
matW %>% head()
options(digits = 3) # 少数点以下3位まで表示
matW %>% head()
###########################よくわからん##########################
# applyの第1引数はデータ（今回はパイプ処理しているからナシ）
# 第2引数は、「１」ならば行単位で処理、「２」ならば列単位で処理。
# 第3引数で処理を指定。行列の列ごとに要素を2倍。
matW %>% apply(2, function(x) sum(x^2))
?docMatrix
#　データフレームからテキスト解析を行う場合、文字列が因子に変換されないように「stringAsFactor=FALSE」とすべし。
photo <- read.csv("data/photo.csv", stringsAsFactor = FALSE)
photo
res <- docMatrixDF(photo[ ,"Reply"]) # photoデータのReplyカラムだけを抽出して、単語文書行列を作成。
# 単語文書行列を作れる関数は「docMatrix()」「docMatrix2()」「docMatrixDF()」のみ。
res #単語文書行列（複数の文書を単語単位で解析した結果を行列で表したもの。）
# docDFは、「Nグラム」を作る関数。
# 「Nグラム」は、単語文書行列（形態素解析の結果をデータフレーム化したもの）になっているデータから作成
# だから、第1引数に単語文書行列のデータを取る。
# 第2引数は、単語文書"行列"なので、どの列のNグラムを作るかを指定。
# 第3引数に、形態素解析（１）か文字単位の解析（０）かを選ぶ。
# Nグラムに使う品詞を「pos」で指定。デフォルトは形態素全部。
res <- docDF(photo, column = "Reply", type = 1, pos = c("名詞","形容詞"), N = 2)
res #名詞・形容詞が2語連続で連なっている組み合わせは、この1つだけだった。
# Sというオブジェクトに「メロスは激怒した」という文章をデータフレーム化して、文字列として代入。
S <- data.frame(BUN = "メロスは激怒した", stringsAsFactors = FALSE)
# データフレーム化したSをでNグラムを作成。「docDF」はNグラムを作る関数。
# pos指定がないので全ての形態素。
# columnは列名でも列番号でもOK。
(docDF(S, column = 1, type = 1, N = 2))
?docDF
## Nグラムを作れる関数は「docDF()」「Ngram()」「NgramDF()」など。
merosu <- Ngram("data/merosu.txt", type = 0, N = 2) # 「type=0」なので、文字単位の解析になる。
merosu %>% head() #助詞と句点で書き手を分類することができる。
merosu <- Ngram("data/merosu.txt", type = 1, N = 2) # 「type=1」なので、形態素単位の解析になる。
merosu %>% head(20) #助詞と句点で書き手を分類することができる。
merosu <- Ngram("data/merosu.txt", type = 2, N = 2) # 「type=2」で品詞を表示。
merosu %>% head()
# 「nDF=1」はTERMで纏められている2語をN1とN2に分けて出力
merosu <- docDF("data/merosu.txt", type = 1, pos = c("名詞","形容詞"), N = 2, nDF = 1)
merosu %>% head(20)
#今までのNグラム出力はデータフレームだった。しかし、分析関数によっては、データ形式が行列でなければならない。
# docNgramはNグラムデータを行列で出力。
merosu <- docNgram("data/doc")
merosu &>& head()
merosu %>% head()
merosu %>% rownames()
merosu <- NgramDF("data/merosu.txt", type = 1, N = 2)
merosu %>% head()
res %>% head(15)
# docNgram2はメモリ改善されている。docNgramとの違いは、デフォルトで「type=1」で文字単位の解析・バイグラム
res <- docNgram2("data/doc") # デフォルトでは文字単位の解析・バイグラムになっている。
res <- docNgram2("data/doc", type = 1, pos = c("名詞","形容詞")) #「type=1」で形態素のバイグラム。
res <- docNgram2("data/doc",type = 1,
pos = c("名詞","形容詞","記号")) # 句読点は「記号」と分類されている。
res %>% head(15)
